{"paragraphs":[{"text":"%sh\npip install pandas numpy scikit-learn imblearn matplotlib seaborn","user":"anonymous","dateUpdated":"2025-05-06T20:57:10+0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","lineNumbers":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1746496093818_-792538927","id":"20250506-084813_1106923434","dateCreated":"2025-05-06T08:48:13+0700","dateStarted":"2025-05-06T20:57:10+0700","dateFinished":"2025-05-06T20:57:11+0700","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6076"},{"text":"%pyspark\r\n\r\n# Cell 0: Import libraries and preprocess data\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, cross_val_score\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.inspection import permutation_importance\r\nfrom sklearn.metrics import f1_score, roc_curve, precision_recall_curve, confusion_matrix\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nfrom sklearn.neural_network import MLPClassifier\r\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\r\nfrom sklearn.linear_model import LogisticRegression\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom imblearn.over_sampling import SMOTE\r\nimport warnings\r\nimport os","user":"anonymous","dateUpdated":"2025-05-06T20:57:11+0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1746496302298_-1651673599","id":"20250506-085142_2029439131","dateCreated":"2025-05-06T08:51:42+0700","dateStarted":"2025-05-06T20:57:11+0700","dateFinished":"2025-05-06T20:57:11+0700","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6077"},{"text":"%pyspark\r\n\r\n# Set random seed for reproducibility\r\nnp.random.seed(42)\r\n\r\n# Create output directory for saving results\r\noutput_dir = 'model_outputs'\r\nif not os.path.exists(output_dir):\r\n    os.makedirs(output_dir)\r\n\r\ndef load_and_preprocess_data(file_path):\r\n    # Load dataset\r\n    data = pd.read_csv(file_path)\r\n\r\n    # Remove duplicates\r\n    print(f\"Initial data shape: {data.shape}\")\r\n    print(f\"Number of duplicates before removal: {data.duplicated().sum()}\")\r\n    data = data.drop_duplicates()\r\n    print(f\"Data shape after removing duplicates: {data.shape}\")\r\n\r\n    # Handle outliers (using IQR for numerical columns)\r\n    numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\r\n    for col in numerical_cols:\r\n        Q1 = data[col].quantile(0.25)\r\n        Q3 = data[col].quantile(0.75)\r\n        IQR = Q3 - Q1\r\n        lower_bound = Q1 - 1.5 * IQR\r\n        upper_bound = Q3 + Q1  # Kept as in original code\r\n        data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]\r\n    print(f\"Data shape after outlier removal: {data.shape}\")\r\n\r\n    # Feature engineering\r\n    data['chol_age_ratio'] = data['chol'] / data['age']\r\n    data['thalach_age_ratio'] = data['thalach'] / data['age']\r\n    data['oldpeak_thalach_ratio'] = data['oldpeak'] / (data['thalach'] + 1e-6)\r\n    data['cp_exang_interaction'] = data['cp'] * data['exang']\r\n\r\n    # Handle oldpeak for log transform\r\n    data['oldpeak'] = data['oldpeak'].clip(lower=1e-6)  # Avoid zero or negative values\r\n    data['log_oldpeak'] = np.log1p(data['oldpeak'])\r\n\r\n    # Convert trestbps to bins (ensure numerical encoding)\r\n    data['trestbps_bin'] = pd.cut(data['trestbps'], bins=[0, 120, 140, 200], labels=[0, 1, 2]).astype(int)\r\n\r\n    # Remove rows with NaN or infinite values\r\n    data = data.replace([np.inf, -np.inf], np.nan)\r\n    data = data.dropna()\r\n    print(f\"Data shape after removing NaN/infinite values: {data.shape}\")\r\n\r\n    # Define categorical and numerical columns\r\n    categorical_cols = ['cp', 'restecg', 'slope']\r\n    numerical_cols = ['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\r\n                     'chol_age_ratio', 'thalach_age_ratio', 'oldpeak_thalach_ratio',\r\n                     'log_oldpeak', 'trestbps_bin', 'cp_exang_interaction']\r\n\r\n    # Preprocessing pipeline\r\n    preprocessor = ColumnTransformer(\r\n        transformers=[\r\n            ('num', StandardScaler(), numerical_cols),\r\n            ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\r\n        ])\r\n\r\n    # Split features and target\r\n    X = data.drop('target', axis=1)\r\n    y = data['target']\r\n\r\n    # Apply preprocessing\r\n    X = preprocessor.fit_transform(X)\r\n\r\n    # Get feature names after preprocessing\r\n    cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\r\n    feature_names = numerical_cols + list(cat_feature_names)\r\n\r\n    X = pd.DataFrame(X, columns=feature_names)\r\n\r\n    # Feature selection based on Random Forest importance (replacing XGBoost)\r\n    rf_temp = RandomForestClassifier(random_state=42)\r\n    rf_temp.fit(X, y)\r\n    feature_importance = pd.Series(rf_temp.feature_importances_, index=feature_names)\r\n    selected_features = feature_importance[feature_importance > 0.02].index.tolist()\r\n    X = X[selected_features]\r\n    print(f\"Selected features: {selected_features}\")\r\n    print(f\"Data shape after feature selection: {X.shape}\")\r\n\r\n    # Apply SMOTE for data augmentation\r\n    smote = SMOTE(random_state=42, k_neighbors=3)\r\n    X, y = smote.fit_resample(X, y)\r\n    print(f\"Data shape after SMOTE: {X.shape}\")\r\n    print(\"Class distribution after SMOTE:\")\r\n    print(y.value_counts())\r\n\r\n    # Split data into train and test sets\r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        X, y, test_size=0.2, random_state=42\r\n    )\r\n\r\n    return X_train, X_test, y_train, y_test, X, y, selected_features\r\n\r\n# Load and preprocess data\r\nfile_path = r'C:\\Users\\anhkh\\OneDrive\\Desktop\\NhapMonKhoaHocDuLieu\\report_ck\\combined_heart_data.csv'\r\nprint(\"=== Preprocessing Data ===\")\r\nX_train, X_test, y_train, y_test, X, y, feature_names = load_and_preprocess_data(file_path)","user":"anonymous","dateUpdated":"2025-05-06T20:57:11+0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":false,"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1746495850399_-946258889","id":"20250506-084410_1321337510","dateCreated":"2025-05-06T08:44:10+0700","dateStarted":"2025-05-06T20:57:11+0700","dateFinished":"2025-05-06T20:57:12+0700","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6078"},{"text":"%pyspark\n\n# Cell 1: Train MLP\ndef train_mlp(X_train, y_train):\n    \"\"\"\n    Train the MLP model using GridSearchCV.\n    Returns the best model and its parameters.\n    \"\"\"\n    print(\"\\nTraining MLP...\")\n    model = MLPClassifier(random_state=42, max_iter=3000, early_stopping=True, tol=1e-3)\n    param_grid = {\n        'hidden_layer_sizes': [(100,), (100, 50), (50, 50)],\n        'activation': ['relu', 'tanh'],\n        'learning_rate_init': [0.001, 0.01],\n        'alpha': [0.0001, 0.001]\n    }\n\n    grid_search = GridSearchCV(\n        model,\n        param_grid,\n        cv=5,\n        scoring='roc_auc',\n        n_jobs=-1\n    )\n    grid_search.fit(X_train, y_train)\n\n    print(f\"Best Parameters: {grid_search.best_params_}\")\n    return grid_search.best_estimator_, grid_search.best_params_\n\n# Train MLP\nmlp_model, mlp_params = train_mlp(X_train, y_train)","user":"anonymous","dateUpdated":"2025-05-06T20:57:12+0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1746497757519_-1736511574","id":"20250506-091557_1088093054","dateCreated":"2025-05-06T09:15:57+0700","dateStarted":"2025-05-06T20:57:12+0700","dateFinished":"2025-05-06T20:57:14+0700","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6079"},{"text":"%pyspark\n# Cell 2: Evaluate and visualize\n\n\ndef evaluate_and_visualize_model(model, model_name, X_train, X_test, y_train, y_test, X, y, feature_names):\n    \"\"\"\n    Evaluate and visualize results for a given model, keeping F1 Score and other evaluation methods.\n    Saves results and plots to the output directory.\n    \"\"\"\n    # Predict on test set\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    # Calculate F1 Score\n    f1 = f1_score(y_test, y_pred)\n    \n    # Print results\n    print(f\"\\n{model_name} Results:\")\n    print(f\"F1 Score: {f1:.4f}\")\n\n    # Classification Report\n    print(\"\\nClassification Report:\")\n    class_report = classification_report(y_test, y_pred, target_names=['No Disease', 'Disease'])\n    print(class_report)\n\n    # Save results to file\n    with open(os.path.join(output_dir, f'model_results_{model_name.lower()}.txt'), 'w') as f:\n        f.write(f\"Model Evaluation Results - {model_name}\\n\")\n        f.write(\"=\"*40 + \"\\n\")\n        f.write(f\"F1 Score: {f1:.4f}\\n\")\n        f.write(\"-\"*40 + \"\\n\")\n        f.write(\"Classification Report:\\n\")\n        f.write(class_report)\n        f.write(\"-\"*40 + \"\\n\")\n\n    # ROC Curve\n    plt.figure(figsize=(12, 8), dpi=200)\n    fpr, tpr, _ = roc_curve(y_test, y_prob)\n    plt.plot(fpr, tpr, label=f'{model_name}')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate', fontsize=12)\n    plt.ylabel('True Positive Rate', fontsize=12)\n    plt.title(f'ROC Curve - {model_name}', fontsize=14)\n    plt.legend(loc='lower right')\n    plt.grid(True)\n    plt.savefig(os.path.join(output_dir, f'roc_curve_{model_name.lower()}.png'))\n    plt.show()\n\n    # Confusion Matrix\n    plt.figure(figsize=(6, 5), dpi=200)\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title(f'Confusion Matrix - {model_name}')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.savefig(os.path.join(output_dir, f'confusion_matrix_{model_name.lower()}.png'))\n    plt.show()\n\n    # Learning Curve\n    train_sizes, train_scores, test_scores = learning_curve(\n        model, X, y, cv=5, scoring='roc_auc', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n    )\n    plt.figure(figsize=(10, 6), dpi=200)\n    plt.plot(train_sizes, train_scores.mean(axis=1), label='Training AUC')\n    plt.plot(train_sizes, test_scores.mean(axis=1), label='Validation AUC')\n    plt.xlabel('Training Examples')\n    plt.ylabel('AUC')\n    plt.title(f'Learning Curve - {model_name}', fontsize=14)\n    plt.legend(loc='best')\n    plt.grid(True)\n    plt.savefig(os.path.join(output_dir, f'learning_curve_{model_name.lower()}.png'))\n    plt.show()\n\n    # Feature Importance using Permutation Importance\n    print(\"\\nComputing Feature Importance...\")\n    perm_importance = permutation_importance(model, X_test, y_test, scoring='roc_auc', n_repeats=10, random_state=42, n_jobs=-1)\n    sorted_idx = perm_importance.importances_mean.argsort()\n    \n    plt.figure(figsize=(10, len(feature_names) * 0.3), dpi=200)\n    plt.barh(np.array(feature_names)[sorted_idx], perm_importance.importances_mean[sorted_idx])\n    plt.xlabel('Permutation Importance (AUC Decrease)')\n    plt.title(f'Feature Importance - {model_name}', fontsize=14)\n    plt.savefig(os.path.join(output_dir, f'feature_importance_{model_name.lower()}.png'))\n    plt.show()\n\n# Evaluate and visualize MLP\nevaluate_and_visualize_model(mlp_model, 'MLP', X_train, X_test, y_train, y_test, X, y, feature_names)","user":"anonymous","dateUpdated":"2025-05-06T20:57:14+0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1746509120025_39323307","id":"20250506-122520_1888524194","dateCreated":"2025-05-06T12:25:20+0700","dateStarted":"2025-05-06T20:57:14+0700","dateFinished":"2025-05-06T20:57:16+0700","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6080"},{"text":"%pyspark\n\n# Evaluate and visualize MLP\nevaluate_and_visualize_model(mlp_model, 'MLP', X_train, X_test, y_train, y_test, X, y, feature_names)\n","user":"anonymous","dateUpdated":"2025-05-06T20:57:16+0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1746497555143_-493683858","id":"20250506-091235_52709663","dateCreated":"2025-05-06T09:12:35+0700","dateStarted":"2025-05-06T20:57:16+0700","dateFinished":"2025-05-06T20:57:17+0700","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6081"},{"text":"%pyspark\r\n\r\n# Cell 3: Train Stacking Classifier\r\ndef train_stacking(X_train, y_train):\r\n    \"\"\"\r\n    Train the Stacking Classifier with base models and a meta-learner.\r\n    Returns the best model and its parameters.\r\n    \"\"\"\r\n    print(\"\\nTraining Stacking Classifier...\")\r\n    # Define base models (replace XGBoost with Random Forest)\r\n    estimators = [\r\n        ('rf', RandomForestClassifier(random_state=42, n_estimators=100, max_depth=3)),\r\n        ('mlp', MLPClassifier(random_state=42, hidden_layer_sizes=(100,), max_iter=7000, tol=1e-5))\r\n    ]\r\n\r\n    # Define meta-learner\r\n    stacking_model = StackingClassifier(\r\n        estimators=estimators,\r\n        final_estimator=LogisticRegression(),\r\n        n_jobs=-1,\r\n        passthrough=True\r\n    )\r\n\r\n    # Grid search for stacking\r\n    param_grid = {\r\n        'final_estimator__C': [0.1, 1.0, 10.0]\r\n    }\r\n\r\n    grid_search = GridSearchCV(\r\n        stacking_model,\r\n        param_grid,\r\n        cv=5,\r\n        scoring='roc_auc',\r\n        n_jobs=-1\r\n    )\r\n    grid_search.fit(X_train, y_train)\r\n\r\n    print(f\"Best Parameters: {grid_search.best_params_}\")\r\n    return grid_search.best_estimator_, grid_search.best_params_\r\n\r\n# Train Stacking Classifier\r\nstacking_model, stacking_params = train_stacking(X_train, y_train)","user":"anonymous","dateUpdated":"2025-05-06T20:57:18+0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1746495911058_1043165951","id":"20250506-084511_629944654","dateCreated":"2025-05-06T08:45:11+0700","dateStarted":"2025-05-06T20:57:18+0700","dateFinished":"2025-05-06T20:58:31+0700","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6082"},{"text":"%pyspark\n# Cell 4: Evaluate and visualize Stacking Classifier\nevaluate_and_visualize_model(stacking_model, 'Stacking', X_train, X_test, y_train, y_test, X, y, feature_names)","user":"anonymous","dateUpdated":"2025-05-06T20:58:31+0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1746495914443_-1337037876","id":"20250506-084514_1922898272","dateCreated":"2025-05-06T08:45:14+0700","dateStarted":"2025-05-06T20:58:31+0700","dateFinished":"2025-05-06T21:00:51+0700","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6083"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2025-05-06T21:00:52+0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1746496435595_1546178734","id":"20250506-085355_668913029","dateCreated":"2025-05-06T08:53:55+0700","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6084"}],"name":"heart","id":"2KVYCRDVU","noteParams":{},"noteForms":{},"angularObjects":{"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}